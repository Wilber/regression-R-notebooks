---
title: 'Introduction to Multiple Linear Regression with R'
author: 'Wilbur Ouma'
date: 'March 20, 2024'
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
editor_options: 
  markdown: 
    wrap: 72
---

## Overview

In this workshop, we will introduce the multiple linear regression model
as an extension of the simple linear regression model, to accommodate
multiple predictors.

Inferences for the multiple linear regression model will be discussed.

Emphasis will be placed on interpreting regression coefficients.

## Objectives

Upon successful completion of this workshop, you should be able to:

-   Fit a multiple linear regression model to data.

-   Use summary statistics from the fit to describe the relationship
    between a response variable and a set of explanatory variables.

-   Interpret model coefficients.

### Model definition

Simple linear regression is a useful approach for predicting a response
on the basis of a single predictor variable. However, in practice we
often have more than one predictor.

A multiple linear model takes the form:

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$

where $X_j$ represents the jth predictor and $\beta_j$ quantifies the
association between that variable and the response.

We interpret $\beta_j$ as the average effect on $Y$ of a one unit
increase in $X_j$, **holding all other predictors fixed.**

As was the case in the simple linear regression setting, the regression
coefficients $\beta_0, \beta_1, ..., \beta_p$ in the above equation are
unknown, and must be estimated.

Given estimates $\hat\beta_0, \hat\beta_1, ..., \hat\beta_p$, we can
make predictions using the formula

$\hat{y} = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2 + ... + \hat\beta_px_p$

The parameters are estimated using the same least squares approach that
we saw in the context of simple linear regression, where we choose
$\beta_0, \beta_1,...,\beta_p$ to minimize the sum of squared residuals,
RSS.

Note that the term ***linear*** in multiple linear regression means
**linear in regression coefficients**, such as:

$Y = \mu\{Y|X_1,X_2\} = \beta_0 + \beta_1X_1 + \beta_2X_2$

$Y = \mu\{Y|X_1\} = \beta_0 + \beta_1X_1 + \beta_2X^2_1$

$Y = \mu\{Y|X_1,X_2\} = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1 X_2$

$Y = \mu\{Y|X_1,X_2\} = \beta_0 + \beta_1log(X_1) + \beta_2log(X_2)$

A non-linear regression model would take a form such as:

$Y = \mu\{Y|X_1\} = \beta_0 \cdot e^{\beta_1X_1}$

### Case Study 1: Why Do Some Mammals Have Large Brains for Their Size?---An Observational Study

Evolutionary biologists are keenly interested in the characteristics
that enable a species to withstand the selective mechanisms of
evolution. An interesting variable in this respect is brain size.

One might expect that bigger brains are better, but certain penalties
seem to be associated with large brains, such as the need for longer
pregnancies and fewer offspring.

We will use data from *G. A. Sacher and E. F. Staffeldt, "Relation of
Gestation Time to Brain Weight for Placental Mammals; Implications for
the Theory of Vertebrate Growth," American Naturalist, 108 (1974):
593--613.* The data can be obtained from the `Sleuth3` R package.

**Research question:** since brain size is obviously related to body
size, the question of interest is --- which, if any, variables are
associated with brain size, **after accounting for body size**?

We first load appropriate $R$ packages:

```{r, warning=FALSE}
library(tidyverse)
library(ISLR2)
library(Sleuth3)
library(MASS)
```

Next, investigate the dataset of 96 species

```{r}
brain_size <- case0902
print.data.frame(head(brain_size))
```

There are three continuous explanatory variables --- body weight (kgs),
gestation period (days), litter size --- and one response variable,
brain weight (grams).

#### Data exploration

We generate a scatterplot matrix to visualize pairwise relationships
between variables to determine whether there's need for data
transformations.

```{r}
pairs(brain_size[,-1], pch = 19, lower.panel = NULL)

```

Pair-wise comparisons of brain weight and other variables suggests need
for log transformation.

```{r}
brain_size$BrainLog<-log(brain_size$Brain)
brain_size$BodyLog<-log(brain_size$Body)
brain_size$GestationLog<-log(brain_size$Gestation)
brain_size$LitterLog<-log(brain_size$Litter)

pairs(brain_size[,6:9], pch = 19, lower.panel = NULL )
```

We then fit multiple linear regression model on the log-transformed
data.

```{r}
fit_brain <- lm(BrainLog ~ BodyLog + LitterLog + GestationLog, data = brain_size)
summary(fit_brain)
```

#### **Inference**

**Research question:** since brain size is obviously related to body
size, the question of interest is --- which, if any, variables are
associated with brain size, after accounting for body size?

The data provide convincing evidence that brain weight was associated
with either gestation length (positive) or litter size (negative),
**even after accounting for the effect of body weight**.

1.  There is strong evidence that litter size was associated with brain
    weight after accounting for body weight and gestation (two-sided
    p-value = 0.0089).

2.  There is strong evidence that gestation period was associated with
    brain weight after accounting for body weight and litter size
    (two-sided p-value = 0:0038).

3.  All the three predictor variables account for about 95% of variation
    observed in the response variable.

All the response variables encountered so far have been continuous. In
the following case study, we encounter a case with a categorical
predictor variable.

### Case Study 2: Does smoking during pregnancy affect birth weight?

**Research question:** *after taking into account length of gestation*,
is there a significant difference in the average birth weights of babies
born to smoking and non-smoking mothers?

Researchers (Daniel, 1999) interested in answering the above research
question collected the following data on a random sample of n = 32
births:

-   Response ($Y$): birth weight of baby ($Wgt$) in grams

-   Potential predictor ($X_1$): length of gestation ($Gest$) in weeks

-   Potential predictor ($X_2$): Smoking status of mother, $Smoke$ (yes
    or no)

We load the data from a public repository:

```{r}
birth_smokers<-read_csv(file = "https://figshare.com/ndownloader/files/31122502", show_col_types = FALSE) 
print.data.frame(head(birth_smokers))
```

```{r}
str(birth_smokers)
```

We then convert the $Smoke$ variable to a factor with two levels:

```{r}
birth_smokers = transform(birth_smokers, Smoke = factor(ifelse(birth_smokers$Smoke == 1, "Smoker", "NonSmoker"))) 

print.data.frame(head(birth_smokers)) 
```

```{r}
str(birth_smokers)
```

We make "NonSmoker" the reference/baseline, such that value 0 is for
smokers when $R$ creates the associated **indicator/dummy variable**:

```{r}
birth_smokers <- birth_smokers %>% 
  mutate(Smoke = relevel(Smoke, ref = "NonSmoker"))
```

#### Qualitative predictors: dummy variables are smart!

In our discussion so far, we have assumed that all variables in our
linear regression model are quantitative. But in practice, this is not
necessarily the case; often some predictors are qualitative (factors).

We will create a dummy (or indicator) variable that takes on two
possible numerical values (levels) of a factor.

For example, based on the $Smoke$ variable, we can create a new variable
that takes the form:

$\mathrm{x_i} = \begin{cases} 1 & \text{if ith mother smoked} \\ 0 & \text{if ith mother did not smoke,} \end{cases}$

and use this variable as a predictor in the regression equation. $R$
will automatically create the indicator variable.

First, let's obtain a scatter-plot matrix of the data:

```{r}
pairs(birth_smokers, pch = 19, lower.panel = NULL)
```

which suggests, not surprisingly, that there is a positive linear
relationship between length of gestation and birth weight. That is, as
the length of gestation increases, the birth weight of babies tends to
increase.

It is hard to see if any kind of (marginal) relationship exists between
birth weight and smoking status, or between length of gestation and
smoking status.

The important question remains --- **after taking into account length of
gestation, is there a significant difference in the average birth
weights of babies born to smoking and non-smoking mothers?**

A model with one binary predictor and one quantitative predictor that
helps us answer the question is:

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon$

And for individual observations:

$y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \epsilon_i$

where:

$y_i$ is the weight of baby i

$x_{i1}$ is length of gestation of mother to baby i

$x_{i2}$ is a binary variable coded as a 1, if the baby's mother smoked
during pregnancy, and 0 if she did not,

and of course the independent error terms $\epsilon_i$ follow a normal
distribution with mean 0 and equal variance $\sigma^2$.

Our dummy variable for $SmokeSmoker$ ($X_2$) becomes:

$\mathrm{x_{i2}} = \begin{cases} 1 & \text{if ith mother smokes} \\ 0 & \text{if ith mother does not smoke,} \end{cases}$

and the resulting regression equation becomes:

$\mathrm{y_i} = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \epsilon_i = \begin{cases} \beta_0 + \beta_1x_{i1} + \beta_2 + \epsilon_i & \text{if ith mother smokes} \\ \beta_0 + \beta_1x_{i1} + \epsilon_i & \text{if ith mother does not smoke,} \end{cases}$

Now we interpret coefficients as follows:

1.  $\beta_1$ is the average change (increase) in the response variable
    (birth weight) for every unit increase in the quantitative predictor
    $X_1$, the gestation length, for both groups (smokers and
    non-smokers).

2.  $\beta_2$ is the average difference in birth weight of babies
    between mothers who smoke and non-smokers, ***after accounting for
    differences due to length of gestation***, i.e for fixed (any)
    values of $X_1$

#### Hypothesis testing and inference

We are now ready to answer the following research question: **is there a
significant difference in mean birth weights for the two groups, after
taking into account length of gestation?**

We test the null hypothesis that $\beta_2 = 0$, against the alternative
$\beta_2 \neq 0$

We fit a multiple linear regression model as shown below:

```{r}
fit_birth <- lm(Wgt ~ Gest + Smoke, data = birth_smokers) 
summary(fit_birth)
```

From the above output, the regression equation becomes:

$Wgt = -2389.57 + 143.10 \cdot Gest - 244.54 \cdot Smoke$

We observe from the output that the p-value associated with $\beta_2$ is
less than 0.001. At just about any significance level, we can reject the
null hypothesis $H_0: \beta_2 = 0$ in favor of the alternative
hypothesis $H_a: \beta_2 \neq 0$.

There is sufficient evidence to conclude that there is a statistically
significant difference in the mean birth weight of all babies of smoking
mothers and the mean birth weight of babies of all non-smoking mothers,
*after taking into account length of gestation*.

In fact, the negative value of $\beta_2$ implies that smoking is
associated with a mean reduction of birth weight of about 245 grams.

### Learning check!

Why don't we test the difference in the mean birth weights for two
groups (smoker vs non-smoker) without regard for the gestation length?

1.  Perform a t-test to determine whether the difference in the average
    birth weights is significant.

    ```{r}
    #t-test example
    a <- 1:30
    b <- (1:30)^2
    t.test(a,b)
    ```

2.  Fit a SLR model of $weight \sim Smoke$. Comment on the regression
    output, i.e. is there a statistically significant relationship
    between birth weight and the smoking status of the mother?

### To fit or not to fit?

Suppose we are just interested in determining the difference in birth
weights for the two groups --- without accounting for changes in birth
weights due to gestation --- we can arrive at a completely different
result.

Performing a two sample t-test on the birth weight data yields the
following:

```{r}
#first, subset the data to obtain birth weights from the Smoker and NonSmoker
wgt_smokers<-birth_smokers[birth_smokers$Smoke=="Smoker", ]$Wgt
wgt_nonSmokers <- birth_smokers[birth_smokers$Smoke=="NonSmoker", ]$Wgt

#box plots to visualize the distribution of the botth weights
boxplot(Wgt ~ Smoke, data = birth_smokers, col = "lightgray")

#perform t-test
t.test(wgt_smokers,wgt_nonSmokers)

```

Performing a SLR of weight on Smoke yields:

```{r}
fit_birth_slr <- lm(Wgt ~ Smoke, data = birth_smokers)
summary(fit_birth_slr)
```

In both tests, we observe no relationship between birth weight smoking
status.

Also, note the proportion of variation in birth weight explained by the
smoking status.

There's therefore need to include potential predictors in a model in
order to account for a majority of variation observed in a response
variable.

### Case Study 3: Survival time for melanoma patients

In some cases of regression modeling, we are interested in determining
**if there's an interaction between predictor variables**. We explore
this in case study 3.

Let us consider Melanoma data set, which is found in the package,
`MASS`. This is a data set of 205 patients in Denmark with malignant
melanoma.

We will focus on three variables:

-   Response ($Y$): survival $time$ in days

-   Potential predictor ($X_1$): $age$ in years

-   Potential predictor ($X_2$): $sex$, (1 = male, 0 = female)

```{r}
?Melanoma
Melanoma$sex[Melanoma$sex == "1"] <- "male"
Melanoma$sex[Melanoma$sex == "0"] <- "female"
print.data.frame(head(Melanoma))
```

**Research question:** does age affect Melanoma survival time
***differently*** for men than women?

Let us explore the data, focusing on the variables of interest --- time,
age, and gender.

We set the reference level for the Sex factor, and make a scatterplot
matrix.

```{r}
Melanoma$sex <- as.factor(Melanoma$sex)
Melanoma$sex <-relevel(Melanoma$sex, ref = "female")
pairs(Melanoma[,c(1,3,4)], pch = 19, lower.panel = NULL)
```

No need for data transformation

We fit a model with an interaction term for age and sex:

$time = \beta_0 + \beta_1 age + \beta_2 sexMale + \beta_3 (age \cdot sexMale)$

and the dummy variable for sex has values:

$\mathrm{sexMale} = \begin{cases} 1 & \text{if ith patient is male} \\ 0 & \text{Otherwise} \end{cases}$

The regression model with a interaction term implies that the effect of
age on survival time is different for male and female patients:

$\mathrm{time_i}=\beta_0 + \beta_1age_i + \begin{cases} \beta_2 + \beta_3age_i & \text{if ith patient is male} \\ 0 & \text{if ith patient is female} \end{cases}$

Upon rearranging:

$\mathrm{time_i}=\begin{cases} (\beta_0 + \beta_2) + (\beta_1 + \beta_3)age_i & \text{if ith patient is male} \\ \beta_0 + \beta_1age_i & \text{if ith patient is female} \end{cases}$

Resulting in different "slopes":

-   The effect of age on survival time, for **male patients**:
    $\beta_1 + \beta_3$

    -   intercept: $\beta_0 + \beta_2$

-   The effect of age on survival time, for **female patients**:
    $\beta_1$

    -   intercept: $\beta_0$

#### Hypothesis testing and inference

To determine interaction, we test the hypothesis that $\beta_3 = 0$

```{r}
#fit a multiple regression model, with interaction term
fit_melanoma = lm(time ~ age + sex + age*sex, data=Melanoma)
summary(fit_melanoma)
```

Notice how the interaction term is marginally significant (p-value
0.0802).

We can also depict the separate slopes:

```{r}
ggplot(Melanoma, aes(x = age, y=time,col=sex)) +
    #  geom_point(alpha=0.4) + 
    geom_point() + geom_smooth(method = "lm", se = FALSE, mapping=aes(y=predict(fit_melanoma,Melanoma))) + 
    #scale_colour_manual(values=c("blue","red")) + 
    labs(x = "Age (years)", y = "Survival Time (days)") +
  ggtitle("Time ~ Age + Sex + Age * Sex") + 
    theme(legend.position = c(0.9, 0.9), legend.title = element_blank()) +
    theme(
        axis.title.x = element_text(size=14),
        axis.title.y = element_text(size=14), 
        axis.text.x = element_text(size = 14), 
        axis.text.y = element_text(size = 14)
    )
```

### Learning Check!

1.  On average, by how much does survival time reduce for a one year
    increase in age, for male patients?

2.  On average, by how much does survival time reduce for a one year
    increase in age, for female patients?

### Conclusion

We can describe the relationship between one response and one or more
(quantitative and/or categorical) predictor variables using a linear
regression model.

However, model assumptions for a linear relationship must be met.

Model coefficients estimate the average effect of a predictor on the
response variable.
